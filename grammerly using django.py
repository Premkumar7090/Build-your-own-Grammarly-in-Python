# -*- coding: utf-8 -*-
"""OpenSource Grammarly API Alternative with Gramformer & FastAPI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pn0jpDENl2FoAWXFdSx49f9xJNpzmxWW
"""

pip install git+https://github.com/PrithivirajDamodaran/Gramformer.git@v0.1 -q

from gramformer import Gramformer
import torch

def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

gf = Gramformer(models = 3, use_gpu=False) # 0=detector, 1=highlighter, 2=corrector, 3=all

PATH = "/content/sample_data/gf.pth"

torch.save(gf, PATH)

gf_inference = torch.load(PATH)

influent_sentences = [
                      "Almost two 1) thousands of years after being 2) baried by falling ash from a two-day volcanic eruption, Pompeii reveals fascinating details about 3) day to day life in the Roman Empire. 4) Pompeii’spopulation roughly 20,000 inhabitants practiced several religions. This is evidenced by temples dedicated to the Egyptian goddess Isis, as well as the 5) presense of Jews and worshipers of Cybele (called the “Great Mother” by her followers). Pompeii’s citizens practiced all of these religions in apparent peaceful coexistence with followers of the state religion, 6) but worshipped Jupiter and the Roman 7) emperor they led astonishingly long lives, assisted by doctors and dentists, and 8) were very well educated"
]

p=



for influent_sentence in influent_sentences:
    corrected_sentence = gf_inference.correct(influent_sentence)
    print("[Input] ", influent_sentence)
    print("[Correction] ",corrected_sentence[0])
    print("-" *100)

influent_sentences = [
    "When I does something, It're always for your good",
    "This isn't how I like to ate food but that's what life give me "
]   

for influent_sentence in influent_sentences:
    corrected_sentence = gf_inference.correct(influent_sentence)
    print("[Input] ", influent_sentence)
    print("[Correction] ",corrected_sentence[0])
    print("-" *100)

!pip install colabcode -q 
!pip install fastapi -q

from colabcode import ColabCode
from fastapi import FastAPI

cc = ColabCode(port=12000, code=False)

app = FastAPI(title="OpenSource Grammarly API Alternative", description="with Gramformer & FastAPI", version="1.0")

# # Initialize logging
# my_logger = logging.getLogger()
# my_logger.setLevel(logging.DEBUG)
# logging.basicConfig(level=logging.DEBUG, filename='logs.log')

gf_inference = None

@app.on_event("startup")
def load_model():
    global gf_inference
    gf_inference = torch.load(PATH)

@app.post("/api/{sentence}")
async def get_predictions(sentence: str):
    try:
       # sentence = "This isn't how I like to ate food but that's what life give me"
        output_sentence = gf_inference.correct(sentence)
        return {"prediction": output_sentence[0]}
    except:
        my_logger.error("Something went wrong!")
        return {"prediction": "error"}

cc.run_app(app=app)

